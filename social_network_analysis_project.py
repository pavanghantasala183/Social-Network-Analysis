# -*- coding: utf-8 -*-
"""Social Network Analysis Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MIoVf9eONvd4W_lwN-0PFx7YfKtbbXMY
"""

# import all the required libraries
import pandas as pd
import numpy as np
import random
import networkx as nx
from tqdm import tqdm
import re
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

# nodes 
with open("fb-pages-food.nodes",'r', encoding='cp850') as f:
    fb_nodes = f.read().splitlines() 

# connections between nodes
with open("fb-pages-food.edges",'r', encoding='cp850') as f:
    fb_links = f.read().splitlines() 

len(fb_nodes), len(fb_links)

fb_nodes

# create a dataframe for all the connections in the form of data frame
node_list_1 = []
node_list_2 = []

for i in tqdm(fb_links):
    node_list_1.append(i.split(',')[0])
    node_list_2.append(i.split(',')[1])

fb_df = pd.DataFrame({'node_1': node_list_1, 'node_2': node_list_2})

fb_df.head()

# create graph
G = nx.from_pandas_edgelist(fb_df, "node_1", "node_2", create_using=nx.Graph())

# plot graph
plt.figure(figsize=(10,10))

pos = nx.random_layout(G, seed=23)
nx.draw(G, with_labels=False,  pos = pos, node_size = 40, alpha = 0.6, width = 0.7)

plt.show()

color_map = {1:'#f09494', 2:'#eebcbc', 3:'#72bbd0', 4:'#91f0a1', 5:'#629fff', 6:'#bcc2f2',  
             7:'#eebcbc', 8:'#f1f0c0', 9:'#d2ffe7', 10:'#caf3a6', 11:'#ffdf55', 12:'#ef77aa', 
             13:'#d6dcff', 14:'#d2f5f0'} 

plt.figure(figsize=(25,25))
options = {
    'edge_color': '#FFDEA2',
    'width': 1,
    'with_labels': True,
    'font_weight': 'regular',
}
#colors = [color_map[G.node[node]['group']] for node in G]
#sizes = [G.node[node]['nodesize']*10 for node in G]

"""
Using the spring layout : 
- k controls the distance between the nodes and varies between 0 and 1
- iterations is the number of times simulated annealing is run
default k=0.1 and iterations=50
"""
nx.draw(G, pos=nx.spring_layout(G, k=0.25, iterations=50), **options)
ax = plt.gca()
ax.collections[0].set_edgecolor("#555555") 
plt.show()

# spring layout graph for aestheti representation of graph (force-anti gravity)
pos = nx.spring_layout(G)

import warnings
warnings.filterwarnings('ignore')

plt.style.use('fivethirtyeight')
plt.rcParams['figure.figsize'] = (20, 15)
plt.axis('off')
nx.draw_networkx(G, pos, with_labels = False, node_size = 35)
plt.show()

# exploring the graph and identifying the important nodes
from operator import itemgetter
def networkStats(G):
    """
    This function prints the basic properties of the Facebook network.
    """
    print(nx.info(G))
    density = nx.density(G)
    print("Network density:", density)
    triadic_closure = nx.transitivity(G)
    print("Triadic closure:", triadic_closure)

    degree_dict = dict(G.degree(G.nodes()))
    nx.set_node_attributes(G, degree_dict, 'degree')
    #print(G.nodes['Kamppi (M)'])

    sorted_degree = sorted(degree_dict.items(), key=itemgetter(1), reverse=True)
    print("\nTop 5 nodes by degree:\n")
    for d in sorted_degree[:5]:
        print(d)

    betweenness_dict = nx.betweenness_centrality(G) # Compute betweenness centrality
    eigenvector_dict = nx.eigenvector_centrality(G) # Compute eigenvector centrality

    sorted_betweenness = sorted(betweenness_dict.items(), key=itemgetter(1), reverse=True)

    print("\nTop 5 nodes by betweenness centrality:\n")
    for b in sorted_betweenness[:5]:
        print(b)

networkStats(G)

# BFS(G,s) returns two dictionaries: the distances and the parents. 
def BFS(G,s):
    
    ## using deque to create a queue object from collections
    queue = deque([s])
    ## initializing the dictionaries
    distance = {s: 0}
    parent = {s: None}
    while queue:
        v = queue.popleft()
        for n in G[v]:
            if n not in distance:            
                queue.append(n)
                # updating the distance of the node
                distance[n] = distance[v] + 1
                # updating the dictionary
                parent[n] = v
    return distance, parent

# number of connected components
def numberOfComponents(G):
    parent = {}
    #Child = []
    count = 0
    for i in G.nodes :
        if i in parent.keys():
            pass
        else:
            count = count + 1 
            distance,parent = BFS(G,i)   
            #n = len(parent) + 1            
            #Child = Child + list(parent.keys()) 
    return count

from collections import deque

BFS(G,'0')

numberOfComponents(G)

!pip install pulp

# Minimum Vertex cover finds the minimum number of vertices covering all the edges in the graph
from pulp import *

def MinimumVertexCover(G):

    ### 
    nodes = list(G.nodes())
    edges = list(G.edges())
    
    # define LP
    prob = LpProblem(name="MinimumVertexCover",sense=LpMinimize)
    
    # variables
    variables = pulp.LpVariable.dicts('nodes',(name for name in nodes),
                                   lowBound=0, upBound=1,
                                   cat=LpInteger)
    
    # constraints 
    for i,j in edges:
        prob += variables[i] + variables[j] >= 1
    
    # objective
    prob += lpSum(variables[i] for i in nodes)
    
    status = prob.solve()
    
    size = value(prob.objective)
    
    sum = 0
    vertices = []
    for i in nodes:
        if variables[i].value() == 1:
          sum = sum + variables[i].value()
          vertices.append(i)

      
    
    return size, vertices

MinimumVertexCover(G)

# Data Preparation
# combine all nodes in a list
node_list = node_list_1 + node_list_2

# remove duplicate items from the list
node_list = list(dict.fromkeys(node_list))

# build adjacency matrix
adj_G = nx.to_numpy_matrix(G, nodelist = node_list)

adj_G

adj_G.shape

#nx.shortest_path_length(G,'0')

# get unconnected node-pairs
all_unconnected_pairs = []

# traverse adjacency matrix
offset = 0
for i in tqdm(range(adj_G.shape[0])):
  for j in range(offset,adj_G.shape[1]):
    if i != j:
      if nx.shortest_path_length(G, str(i), str(j)) <=2:
        if adj_G[i,j] == 0:
          all_unconnected_pairs.append([node_list[i],node_list[j]])

  offset = offset + 1

len(all_unconnected_pairs)

node_1_unlinked = [i[0] for i in all_unconnected_pairs]
node_2_unlinked = [i[1] for i in all_unconnected_pairs]

data = pd.DataFrame({'node_1':node_1_unlinked, 
                     'node_2':node_2_unlinked})

# add target variable 'link'
data['link'] = 0

initial_node_count = len(G.nodes)

fb_df_temp = fb_df.copy()

# empty list to store removable links
redundant_links_index = []

for i in tqdm(fb_df.index.values):
  
  # remove a node pair and build a new graph
  G_temp = nx.from_pandas_edgelist(fb_df_temp.drop(index = i), "node_1", "node_2", create_using=nx.Graph())
  
  # check there is no spliting of graph and number of nodes is same
  if (nx.number_connected_components(G_temp) == 1) and (len(G_temp.nodes) == initial_node_count):
    redundant_links_index.append(i)
    fb_df_temp = fb_df_temp.drop(index = i)

len(redundant_links_index)

# create dataframe of removable edges
fb_df_ghost = fb_df.loc[redundant_links_index]

# add the target variable 'link'
fb_df_ghost['link'] = 1

data = data.append(fb_df_ghost[['node_1', 'node_2', 'link']], ignore_index=True)

data['link'].value_counts()

# drop removable edges
fb_df_partial = fb_df.drop(index=fb_df_ghost.index.values)

# build graph
G_data = nx.from_pandas_edgelist(fb_df_partial, "node_1", "node_2", create_using=nx.Graph())

!pip install node2vec

from node2vec import Node2Vec

# Generate walks
node2vec = Node2Vec(G_data, dimensions=100, walk_length=16, num_walks=50)

# train node2vec model
n2w_model = node2vec.fit(window=7, min_count=1)

x = [(n2w_model[str(i)]+n2w_model[str(j)]) for i,j in zip(data['node_1'], data['node_2'])]

xtrain, xtest, ytrain, ytest = train_test_split(np.array(x), data['link'], 
                                                test_size = 0.3, 
                                                random_state = 35)

len(xtrain)

lr = LogisticRegression(class_weight="balanced")

lr.fit(xtrain, ytrain)

predictions = lr.predict_proba(xtest)

roc_auc_score(ytest, predictions[:,1])

lr.score(xtest,ytest)

# Compute ROC curve and ROC area for each class
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
preds = predictions[:,1]
fpr, tpr, threshold = roc_curve(ytest, preds)
roc_auc = auc(fpr, tpr)

plt.title('Receiver Operating Characteristic - Logistic Regression')
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import export_text
decision_tree = DecisionTreeClassifier(max_depth=10)
decision_tree = decision_tree.fit(xtrain, ytrain)
r = export_text(decision_tree, feature_names=range(100))
print(r)

predictions = decision_tree.predict_proba(xtest)

roc_auc_score(ytest, predictions[:,1])

decision_tree.score(xtest,ytest)

# Compute ROC curve and ROC area for each class
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
preds = predictions[:,1]
fpr, tpr, threshold = roc_curve(ytest, preds)
roc_auc = auc(fpr, tpr)

plt.title('Receiver Operating Characteristic - Decision Tree')
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score
SVMmodel = LinearSVC()
# training
SVMmodel.fit(xtrain, ytrain)
y_pred_SVM = SVMmodel.predict(xtest)
# evaluation
acc_SVM = accuracy_score(ytest, y_pred_SVM)
print("SVM model Accuracy: {:.2f}%".format(acc_SVM*100))

print(roc_auc_score(ytest, y_pred_SVM))

# Compute ROC curve and ROC area for each class
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
preds = y_pred_SVM
fpr, tpr, threshold = roc_curve(ytest, preds)
roc_auc = auc(fpr, tpr)

plt.title('Receiver Operating Characteristic-SVM')
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
DTmodel = DecisionTreeClassifier()
RFmodel = RandomForestClassifier(n_estimators=50, max_depth=3, bootstrap=True, random_state=0) ## number of trees and number of layers/depth
# training
DTmodel.fit(xtrain, ytrain)
y_pred_DT = DTmodel.predict(xtest)
RFmodel.fit(xtrain, ytrain)
y_pred_RF = RFmodel.predict(xtest)
# evaluation
acc_DT = accuracy_score(ytest, y_pred_DT)
print("Decision Tree Model Accuracy: {:.2f}%".format(acc_DT*100))
print(roc_auc_score(ytest, y_pred_DT))
acc_RF = accuracy_score(ytest, y_pred_RF)
print("Random Forest Model Accuracy: {:.2f}%".format(acc_RF*100))
print(roc_auc_score(ytest, y_pred_RF))

## Neural Network and Deep Learning
from sklearn.neural_network import MLPClassifier
DLmodel = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(10,10,10), random_state=1)
# training
DLmodel.fit(xtrain, ytrain)
y_pred_DL= DLmodel.predict(xtest)
# evaluation
acc_DL = accuracy_score(ytest, y_pred_DL)
print("DL model Accuracy: {:.2f}%".format(acc_DL*100))
print(roc_auc_score(ytest, y_pred_DL))

# Compute ROC curve and ROC area for each class
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
preds = y_pred_DL
fpr, tpr, threshold = roc_curve(ytest, preds)
roc_auc = auc(fpr, tpr)

plt.title('Receiver Operating Characteristic - Neural Network')
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

from sklearn.cluster import KMeans
Kmean = KMeans(n_clusters=4)
Kmean.fit(xtrain)

Kmean.cluster_centers_

!pip install pulp

nx.average_shortest_path_length(G)

nx.draw_networkx(nx.minimum_spanning_tree(G))

print(nx.info(G))